{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Learning Machines\n",
    "## Exercise 5 - Convolutional ANN and Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal of this exercise is for you to get a better understanding of what convolution is, how it is leveraged to increase the usability and performance of neural networks. The exercise will also teach you about transfer learning and the differences between fine-tuning/feature extraction. \n",
    "\n",
    "## Literature\n",
    "This exercise will rely on the following sections in the [course book](https://www.deeplearningbook.org/).\n",
    "\n",
    "- Chapter 9\n",
    "    - Most of it\n",
    "- Chapter 7\n",
    "    - Section 7.4 - Dataset augmentation\n",
    "- Chapter 15\n",
    "    - Section 15.2 - Transfer learning\n",
    "    \n",
    "## Examination\n",
    "Epochs are predefined to be 30. Feel free to increase/decrease this number depending on the hardware that you are working with. Just make sure that you use the same hyperparameters on tasks 2, 3 and 4. **Make sure you have all examination requirements in order before presenting.**\n",
    "\n",
    "### Task 1\n",
    "1. Implementation of same convolution.\n",
    "2. The resulting image using 3 different filters.\n",
    "\n",
    "### Task 2\n",
    "1. The given network trained, validated and tested on the given dataset. Don't forget to make the train/validation/test split of the dataset. This can be achieved programmatically using https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split.\n",
    "2. Some type of regularization should be used. You should understand how the chosen regularization technique works.\n",
    "3. Report the training, validation and test accuracy. (Should beat randomly picking)\n",
    "4. Calculate and plot the multi-class [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "5. Add some augmentation techniques which fits well with the data. Does this increase or decrease the validation accuracy?\n",
    "\n",
    "### Task 3\n",
    "1. Fine-tune Resnet18 on the given dataset.\n",
    "2. Report the training, validation and test accuracy. (Should beat randomly picking)\n",
    "3. Calculate and plot the multi-class [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "4. Add some augmentation techniques which fits well with the data. Does this increase or decrease the validation accuracy?\n",
    "\n",
    "### Task 4\n",
    "1. Use Resnet18 as a feature extractor on the dataset.\n",
    "2. Report the training, validation and test accuracy. (Should beat randomly picking)\n",
    "3. Calculate and plot the multi-class [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "4. Add some augmentation techniques which fits well with the data. Does this increase or decrease the validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "\n",
    "# Confusion matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "def confusion_matrix(model, test_loader, model_path='best_model.pth', class_labels=[0, 1]):\n",
    "    \n",
    "    # Load the trained model weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Disable gradient calculation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = metrics.confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    # Set up the figure with a proper aspect ratio\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))  # Adjust figure size\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    cm_display.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "    # Adjust layout for a cleaner look\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution in Neural Networks\n",
    "A convolutional neural network, CNN for short, is a type of ANN that consists of at least one convolutional layer. CNN's are often used where the input size may vary such as when we are dealing with image input. The architecture of CNNs was inspired by how the visual cortex functions in our brain.\n",
    "\n",
    "## Task 1: Implement convolution\n",
    "Implement 2d same convolution without using a built-in convolution function. This should function as described in [this blog post](https://jcbgamboa.github.io/2017/08/12/what-are-convolutions/). One of the great strengths of convolution is that it functions on any sized image, hence it is important that your implementation also does. Same convolution means that the dimensions of the output are the same as the dimensions of the input. This is achieved by padding the input.\n",
    "\n",
    "Once you have implemented a function that performs 2d convolution, use that to perform convolution over all channels in this image. Show the result using 3 different filters.\n",
    "\n",
    "To find the padding needed to get the input to be the same space as the output you can use the formula:\n",
    "\n",
    "$$ n_{out} = \\left \\lfloor\\frac{n_{in}+2p-k}{s} \\right \\rfloor+1 $$\n",
    "\n",
    "where $n_{out}$ is the number of output features, $n_{in}$ is the number of input features, $k$ is the kernel size, $p$ is the padding size and $s$ is the stride size. You can assume that the stride is always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 30]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Implement same convolution in the function below (kernel is a 2d numpy array an example of which can be found in the test)\n",
    "\n",
    "def conv(image, kernel, strides=1):\n",
    "    # Get dimensions\n",
    "    img_h, img_w = image.shape\n",
    "    k_h, k_w = kernel.shape\n",
    "    \n",
    "    #Check padding size\n",
    "    pad_h = k_h // 2\n",
    "    pad_w = k_w // 2\n",
    "\n",
    "    # Add padding\n",
    "    image = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "    # Output dimensions\n",
    "    out_h = math.floor((img_h - k_h+2*pad_h) / strides) + 1\n",
    "    out_w = math.floor((img_w - k_w+2*pad_w) / strides) + 1\n",
    "\n",
    "    \n",
    "    # Initialize output\n",
    "    output = np.zeros((out_h, out_w))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(0, out_h):\n",
    "        for j in range(0, out_w):\n",
    "            region = image[i * strides: i * strides + k_h, j * strides: j * strides + k_w]\n",
    "            output[i, j] = np.sum(region * kernel)  # Element-wise multiplication & summation\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Our test, don't edit\n",
    "inp = np.array([[1,1,1,1],[1,1,2,1],[1,-3,-4,1],[1,1,1,1]])\n",
    "kernel = np.array([[0,1,0],[1,2,1],[0,1,0]]) # This is the second input of conv()\n",
    "\n",
    "# If all are TRUE the convolution is implemented correctly\n",
    "ans = np.array([[4, 5, 6, 4], [5, 3, 3, 6], [1, -7, -7, 0], [4, 1, 0, 4]])\n",
    "print(conv(inp, kernel) == ans)\n",
    "\n",
    "f, axarr = plt.subplots(4,1)\n",
    "\n",
    "# How to load images using opencv\n",
    "image_path = \"ANN3_dataset/101_ObjectCategories_2classes/anchor/image_0001.jpg\" # add your file path here\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # gray scale so we dont have to deal with more than 1 channel\n",
    "\n",
    "#Padding\n",
    "\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "# Define your 3 kernels\n",
    "kernel_1 = np.array([[ 0, -1,  0],[-1,  5, -1], [ 0, -1,  0]]) \n",
    "kernel_2 = np.array([[ -1, -1,  -1],[-1,  8, -1],[ -1, -1,  -1]])\n",
    "kernel_3 = (1/16) * np.array([[1, 2, 1],[2, 4, 2],[1, 2, 1]])\n",
    "\n",
    "\n",
    "# Perform the convolution (might take a couple of seconds depending on the implementation)\n",
    "output1 = conv(image, kernel_1)\n",
    "output2 = conv(image, kernel_2)\n",
    "output3 = conv(image, kernel_3)\n",
    "\n",
    "# plot the loaded image and the 3 convoluted images\n",
    "axarr[0].imshow(image, cmap=\"gray\")\n",
    "axarr[1].imshow(output1, cmap=\"gray\")\n",
    "axarr[2].imshow(output2, cmap=\"gray\")\n",
    "axarr[3].imshow(output3, cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "Computer vision (CV) is a task within the computer science field that aim is to extract high-level information from static images or video. Such high-level information can be, but is not limited to:\n",
    "* Object detection - Detect and classify objects within input images\n",
    "* Anomaly detection - Detect anomalies in the input images\n",
    "* Semantic segmentation - Classify each pixel in the input image into different classes\n",
    "* Object recognition - Classifying an entire image depending on what it contains\n",
    "\n",
    "CV has been studied for multiple decades where early solutions used handwritten feature extractors to extract information from the input. However, with the increase of computing power together with the rise of deep learning algorithms, the main method used to solve CV problems is convolutional neural networks.\n",
    "\n",
    "In this exercise, we will be taking a closer look at object recognition by first using a randomly initialized network and then utilizing transfer learning. **The dataset we will use for this exercise can be downloaded on canvas**. It is a subset of [this dataset](http://www.vision.caltech.edu/Image_Datasets/Caltech101/). Remember to split the data into separate training, validation and test set.\n",
    "\n",
    "## Task 2: Implement the missing code and train it on the given dataset.\n",
    "For task 2, implement the missing parts of the code below. The code should correctly train, validate and test the model. There are some comments guiding you through the process, however if something is unclear try to leverage the documentation for pytorch found [here](https://pytorch.org/docs/stable/index.html). You should also add some type of regularization into your model.\n",
    "\n",
    "Remember to check the examination requirements in the start of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import torchvision.transforms as v2\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120)\n",
    "        self.fc2 = nn.Linear(120, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # 3x64x64 -> 6x60x60 -> 6x30x30\n",
    "        x = self.pool(F.relu(self.conv2(x))) # 6x30x30 -> 16x26x26 -> 16x13x13\n",
    "        x = x.view(-1, 16 * 13 * 13) # Flatten\n",
    "        x = F.relu(self.fc1(x)) # 16x13x13 -> 120\n",
    "        x = self.fc2(x) # 120 -> 2\n",
    "        return x\n",
    "\n",
    "# Implement a train model function so you can re_use it in task 3 and 4. \n",
    "# Should return the best performing model after training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    # Define loss function and optimizer\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss_array = []\n",
    "    # Training loop\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            print\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "        train_loss.append(running_loss/len(train_loader))\n",
    "        \n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            # Save the best model\n",
    "            \n",
    "            \n",
    "\n",
    "            # Save the best model based on validation accuracy\n",
    "            if epoch == 0 or (100 * correct / total) > best_accuracy:\n",
    "                best_accuracy = 100 * correct / total\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if os.path.exists('best_model.pth'):\n",
    "                    os.remove('best_model.pth')\n",
    "                torch.save(best_model_wts, 'best_model.pth')\n",
    "                \n",
    "                \n",
    "        val_loss_array.append(val_loss/len(val_loader))\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\")\n",
    "    print(f\"Best validation accuracy: {best_accuracy}%\")\n",
    "\n",
    "\n",
    "def test_model(model , test_loader):\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test accuracy: {100 * correct / total}%\")\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE = True\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Train augmentations\n",
    "transforms = transforms.Compose([\n",
    "    # Add training augmentations here, remember: we do not want to transform the validation images.\n",
    "    # For information about augmentation see: https://pytorch.org/vision/stable/transforms.html\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip images with 50% chance\n",
    "    transforms.RandomRotation(20),  # Rotate up to 20 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust color properties\n",
    "    transforms.RandomAffine(degrees=0, shear=10),  # Shear transformation\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the full dataset, perform the training/validation/test split and then load the subsets into dataloaders.\n",
    "# Remember that the training images should be augmentated.\n",
    "DATA_DIR = \"ANN3_dataset/101_ObjectCategories_2classes/\" # Path to dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(DATA_DIR, transform=None)\n",
    "\n",
    "validation_split = 0.25\n",
    "train_split = 0.6\n",
    "test_split = 0.15\n",
    "\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Define the sizes for each split\n",
    "train_size = int(train_split * len(dataset))\n",
    "val_size = int(validation_split * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "print(f'Split sizes: train={train_size}, validation={val_size}, test={test_size}')\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_split, validation_split, test_split], generator=generator)\n",
    "\n",
    "\n",
    "\n",
    "# Create the dataloaders\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "\n",
    "\n",
    "\n",
    "# Load our network\n",
    "model = Net()\n",
    "# Apply the transformations to the training dataset\n",
    "train_dataset.dataset.transform = transforms\n",
    "\n",
    "# Apply the transformations to the validation and test datasets\n",
    "val_dataset.dataset.transform = v2.transforms.Compose([v2.transforms.ToTensor()])\n",
    "test_dataset.dataset.transform = v2.transforms.Compose([v2.transforms.ToTensor()])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader, val_loader, epochs)\n",
    "\n",
    "# Test the model\n",
    "tested_model = test_model(model,test_loader)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "confusion_matrix(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "Transfer learning refers to the practice to use a model which has already been pre-trained on a large dataset to be able to solve task $T_1$, replace the output layer or a few of the upper layers within this model and retrain the model on a smaller dataset to be able to solve task $T_2$. Formally this can be described as the following:\n",
    "\n",
    "__Def 1:__ Let $D_s$ be the source domain and $T_s$ be the corresponding source task. Let $D_t$ be the target domain and $T_t$ be the corresponding target task. Let $f_t$ be the predictive function for $T_s$. Thus transfer learning aims to improve the learning of $f_t$ in $D_t$ using the already learned knowledge in $D_s$ and $T_s$ where $D_s \\neq D_t$ and $T_s \\neq T_t$.\n",
    "\n",
    "The benefit from using transfer learning is that we can train an accurate computer vision model with relatively small amounts of data and computing resources compared to the costly pretraining process of the full convolutional neural network (a few days using multiple GPUs). \n",
    "\n",
    "## Fine-tuning and Feature extraction\n",
    "There are two main ideas when it comes to transfer learning, fine-tuning and feature extraction. When using fine-tuning we allow all weights to be changed during the training phase. However, when we use the pre-trained model as a feature extractor we instead freeze earlier layers of the model, which means that the weights in those layers will not be updated during the training phase and we only update the weights in the upper layers that we have replaced. \n",
    "\n",
    "This works because low-level information extracted from the input image is universal between tasks, examples of such information is edge detection, shape detection and pattern detection. This is what the early layers are optimized to do, where later layers extract more abstract features relevant for the task. \n",
    "\n",
    "Most of the pre-trained models in PyTorch are trained on [ImageNet](http://www.image-net.org/). \n",
    "\n",
    "In this exercise, we use ResNet18 as our model. You should make yourself familiar with the Resnet18 architecture using, for example, [the paper](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "## Task 3: Fine-tuning\n",
    "In task 3 you should fine-tune Resnet18 to the small dataset which is provided above. Some code has been given to you. Remember to re-use functions (such as trained_model) from task 2 to decrease the implementation time.\n",
    "\n",
    "Remember to check the examination requirements at the start of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "# Fine-tune a model to the dataset\n",
    "# We use resnet18 as the model.\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features \n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Do the things required for fine-tuning before training the model\n",
    "criterion_ft = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "trained_model_ft = train_model(model_ft, criterion_ft, optimizer_ft, train_loader, val_loader, epochs)\n",
    "\n",
    "# Test the model\n",
    "tested_model_ft = test_model(model_ft,test_loader)\n",
    "\n",
    "#confusion matrix\n",
    "confusion_matrix(model_ft, test_loader)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Feature extraction\n",
    "In task 4, you should use Resnet18 as a feature extractor. Similarly to task 3, some code has been provided. Remember to re-use as much code as you can. \n",
    "\n",
    "Once again, check the examination requirements so you don't forget to implement some required functionality.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How freeezing layers affect the model\n",
    "\n",
    "What freezing layers in a model affects:\n",
    "\n",
    "1. This will prevent weight updates in the frozen layers which means it, no change in gradients, which means unchanged from the pretrained model.\n",
    "2. Uses the model as a Feature Extractor \n",
    "-   For the model that we use (ResNet-18) it retains learned parameters from more than a million images from the ImageNet database \n",
    "3. As the model is already trained, the use of freezing layers will heavily reduce Computational Cost & Training Time\n",
    "4. It can also reduces risk of overfitting as we are not even training on the same data\n",
    "5. Only the Final Layer Learns Task-Specific Features\n",
    "- The newly added fully connected layer is randomly initialized and learns from scratch. It is the only trainable part of your model, meaning it must effectively map extracted features to the new classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a predefined model as a feature extractor\n",
    "\n",
    "# We use resnet18 as the model.\n",
    "model_fe = models.resnet18(pretrained=True)\n",
    "\n",
    "# Do the things required for fine-tuning before training the model\n",
    "\n",
    "# Freeze all layers except the final fully connected layer\n",
    "for param in model_fe.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer to match the number of classes (e.g., num_classes = 10)\n",
    "num_ftrs = model_fe.fc.in_features\n",
    "model_fe.fc = nn.Linear(num_ftrs, 2)  # Adjust num_classes accordingly\n",
    "\n",
    "# Define criterion, optimizer, and loaders\n",
    "criterion_fe = nn.CrossEntropyLoss()\n",
    "optimizer_fe = torch.optim.Adam(model_fe.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "trained_model_fe = train_model(model_fe, criterion_fe, optimizer_fe, train_loader, val_loader, epochs)\n",
    "\n",
    "# Test the model\n",
    "tested_model = test_model(model_fe,test_loader)\n",
    "\n",
    "#confusion matrix\n",
    "confusion_matrix(model_fe, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
